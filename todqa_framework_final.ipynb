{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio import mask\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.features import geometry_mask\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from scipy.stats import wasserstein_distance\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define target CRS for all analysis (UTM Zone 30N for Ghana/Accra)\n",
    "TARGET_CRS = \"EPSG:32630\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import wasserstein_distance\n",
    "from shapely.geometry import box, Point\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# GBA settings\n",
    "TARGET_CRS = \"EPSG:32630\"  # UTM zone 30N for Accra, Ghana\n",
    "\n",
    "class TODQAAssessor:\n",
    "    \"\"\"\n",
    "    Task-Oriented Data Quality Assessment (Li et al., 2019)\n",
    "    Modified for urban building datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=16, random_state=42):\n",
    "        self.k = k  # number of LSH hyperplanes\n",
    "        self.hyperplanes = None\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    def fit_hyperplanes(self, feature_dim: int):\n",
    "        \"\"\"Initialize k random hyperplanes for LSH\"\"\"\n",
    "        self.hyperplanes = np.random.randn(self.k, feature_dim)\n",
    "    \n",
    "    def hash_vector(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute LSH hash for a feature vector\"\"\"\n",
    "        return (np.dot(self.hyperplanes, x) >= 0).astype(int)\n",
    "    \n",
    "    def compute_relevancy(self, D_features: np.ndarray, S_features: np.ndarray, \n",
    "                         task_type: str = 'accessibility') -> float:\n",
    "        \"\"\"\n",
    "        Compute task relevancy score q_D|S_tr\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        D_features : array-like, feature vectors from Open Buildings dataset\n",
    "        S_features : array-like, feature vectors from task sample\n",
    "        task_type : str, type of similarity function to use\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        q_score : float, task relevancy score (0-1)\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(D_features) == 0 or len(S_features) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        if self.hyperplanes is None:\n",
    "            self.fit_hyperplanes(D_features.shape[1])\n",
    "        \n",
    "        # Initialize buckets\n",
    "        buckets = {}\n",
    "        \n",
    "        # Hash D features\n",
    "        for i, x in enumerate(D_features):\n",
    "            hash_val = tuple(self.hash_vector(x))\n",
    "            bucket = buckets.setdefault(hash_val, {'D': [], 'S': []})\n",
    "            bucket['D'].append(x)\n",
    "        \n",
    "        # Hash S features\n",
    "        for i, x in enumerate(S_features):\n",
    "            hash_val = tuple(self.hash_vector(x))\n",
    "            bucket = buckets.setdefault(hash_val, {'D': [], 'S': []})\n",
    "            bucket['S'].append(x)\n",
    "        \n",
    "        # Compute bucket-level similarities\n",
    "        total_similarity = 0.0\n",
    "        total_weight = 0\n",
    "        \n",
    "        for bucket_data in buckets.values():\n",
    "            D_prime = np.array(bucket_data['D'])\n",
    "            S_prime = np.array(bucket_data['S'])\n",
    "            \n",
    "            if len(D_prime) > 0 and len(S_prime) > 0:\n",
    "                # Compute similarity based on task type\n",
    "                if task_type == 'accessibility':\n",
    "                    similarity = self._accessibility_similarity(D_prime, S_prime)\n",
    "                elif task_type == 'morphology':\n",
    "                    similarity = self._morphology_similarity(D_prime, S_prime)\n",
    "                elif task_type == 'density':\n",
    "                    similarity = self._density_similarity(D_prime, S_prime)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "                \n",
    "                total_similarity += similarity * len(D_prime)\n",
    "                total_weight += len(D_prime)\n",
    "        \n",
    "        return total_similarity / total_weight if total_weight > 0 else 0.0\n",
    "    \n",
    "    def _accessibility_similarity(self, D_prime: np.ndarray, S_prime: np.ndarray) -> float:\n",
    "        \"\"\"Compute accessibility similarity in a bucket\"\"\"\n",
    "        if len(D_prime) == 0 or len(S_prime) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Use Earth Mover's Distance (Wasserstein distance)\n",
    "        D_flat = D_prime.flatten()\n",
    "        S_flat = S_prime.flatten()\n",
    "        \n",
    "        # Filter out zero values\n",
    "        D_filtered = D_flat[D_flat > 0]\n",
    "        S_filtered = S_flat[S_flat > 0]\n",
    "        \n",
    "        if len(D_filtered) == 0 or len(S_filtered) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Normalize to 0-1\n",
    "        D_norm = D_filtered / 100.0\n",
    "        S_norm = S_filtered / 100.0\n",
    "        \n",
    "        try:\n",
    "            emd = wasserstein_distance(D_norm, S_norm)\n",
    "            # Convert to similarity score (0-1)\n",
    "            similarity = np.exp(-5 * emd)\n",
    "            return similarity\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _morphology_similarity(self, D_prime: np.ndarray, S_prime: np.ndarray) -> float:\n",
    "        \"\"\"Compute morphological similarity in a bucket\"\"\"\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        if len(D_prime) == 0 or len(S_prime) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Use cosine similarity for morphological features\n",
    "        sim_matrix = cosine_similarity(D_prime, S_prime)\n",
    "        return np.mean(sim_matrix)\n",
    "    \n",
    "    def _density_similarity(self, D_prime: np.ndarray, S_prime: np.ndarray) -> float:\n",
    "        \"\"\"Compute density similarity in a bucket\"\"\"\n",
    "        if len(D_prime) == 0 or len(S_prime) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Use mean absolute percentage similarity\n",
    "        mean_diff = np.abs(np.mean(D_prime) - np.mean(S_prime)) / np.mean(S_prime)\n",
    "        similarity = np.exp(-mean_diff)\n",
    "        return similarity\n",
    "\n",
    "\n",
    "class AccessibilityPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline for computing accessibility metrics for health and water\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, city: str = \"accra\", target_resolution: int = 30):\n",
    "        self.city = city\n",
    "        self.crs = TARGET_CRS\n",
    "        self.target_resolution = target_resolution\n",
    "        self.todqa_assessor = TODQAAssessor(k=16)\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all required datasets\"\"\"\n",
    "        print(\"Loading datasets...\")\n",
    "        \n",
    "        from pathlib import Path\n",
    "        PROJECT_ROOT = Path(\"/headless/EXPTS\")\n",
    "        base_dir = PROJECT_ROOT / \"New_TODQA\" / self.city.lower()\n",
    "\n",
    "        tif_file1 = \"health_accessibility_10\\n.tif\".strip()\n",
    "        tif_path1 = f\"/headless/EXPTS/New_TODQA/accra/{tif_file1}\"\n",
    "\n",
    "        tif_file2 = \"elevation_srtm2014.tif\"\n",
    "        tif_path2 = f\"/headless/EXPTS/New_TODQA/accra/{tif_file2}\"\n",
    "\n",
    "        tif_file3 = \"accra_worldpop2023.tif\"\n",
    "        tif_path3 = f\"/headless/EXPTS/New_TODQA/accra/{tif_file3}\"\n",
    "\n",
    "        # Define paths - ADDED GBA Building Atlas\n",
    "        paths = {\n",
    "            \"boundary\": base_dir / \"ashaiman_boundary\" / \"Ashaiman_boundary.shp\",\n",
    "            \"google_obd\": base_dir / \"google_buildings.geojson\",\n",
    "            \"microsoft_obd\": base_dir / \"microsoft_buildings.geojson\",\n",
    "            \"osm_buildings\": base_dir / \"osm_buildings.gpkg\",\n",
    "            \"global_buildings\": base_dir / \"gba_buildings.gpkg\", #\"global_buildings.geojson\",  # ADDED\n",
    "            \"pipe_network\": base_dir / \"ashaiman_pipeline\" / \"pipe_network.shp\",\n",
    "            \"health_accessibility\": tif_path1,\n",
    "            \"elevation_raster\": tif_path2,\n",
    "            \"population_raster\": tif_path3,\n",
    "        }\n",
    "        \n",
    "        # Load boundary\n",
    "        self.boundary = gpd.read_file(paths['boundary']).to_crs(self.crs)\n",
    "        \n",
    "        # Create analysis boundary with buffer\n",
    "        bounds = self.boundary.total_bounds\n",
    "        centroid = self.boundary.unary_union.centroid\n",
    "        corners = [Point(bounds[0], bounds[1]), Point(bounds[2], bounds[1]),\n",
    "                  Point(bounds[0], bounds[3]), Point(bounds[2], bounds[3])]\n",
    "        distances = [corner.distance(centroid) for corner in corners]\n",
    "        buffer_distance = max(distances) * 0.1\n",
    "        self.analysis_boundary = box(*bounds).buffer(buffer_distance)\n",
    "        self.bounds = self.analysis_boundary.bounds\n",
    "        \n",
    "        # Calculate target dimensions\n",
    "        minx, miny, maxx, maxy = self.bounds\n",
    "        self.width = int((maxx - minx) / self.target_resolution)\n",
    "        self.height = int((maxy - miny) / self.target_resolution)\n",
    "        self.target_transform = rasterio.transform.from_bounds(\n",
    "            minx, miny, maxx, maxy, self.width, self.height\n",
    "        )\n",
    "        \n",
    "        # Load OBD datasets - ADDED GBA Building Atlas\n",
    "        self.google_obd = self._load_and_clip(paths['google_obd'])\n",
    "        self.microsoft_obd = self._load_and_clip(paths['microsoft_obd'])\n",
    "        self.osm_buildings = self._load_and_clip(paths['osm_buildings'])\n",
    "        self.global_buildings = self._load_and_clip(paths['global_buildings'])  # ADDED\n",
    "        \n",
    "        # Load pipe network\n",
    "        self.pipe_network = self._load_and_clip(paths['pipe_network'])\n",
    "        \n",
    "        # Load rasters\n",
    "        self.health_access = self._load_and_align_raster(paths['health_accessibility'])\n",
    "        \n",
    "        if os.path.exists(paths['elevation_raster']):\n",
    "            self.elevation = self._load_and_align_raster(paths['elevation_raster'])\n",
    "        else:\n",
    "            self.elevation = None\n",
    "            \n",
    "        if os.path.exists(paths['population_raster']):\n",
    "            self.population = self._load_and_align_raster(paths['population_raster'])\n",
    "        else:\n",
    "            self.population = None\n",
    "        \n",
    "        print(f\"Data loaded: Google={len(self.google_obd)}, Microsoft={len(self.microsoft_obd)}, \"\n",
    "              f\"OSM={len(self.osm_buildings)}, GBA={len(self.global_buildings)}\")\n",
    "        \n",
    "        boundary_path = \"ranking_results/ashaiman_boundary.geojson\"\n",
    "        self.boundary.to_file(boundary_path, driver='GeoJSON')\n",
    "        print(f\"Boundary saved to {boundary_path}\")        \n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _load_and_clip(self, filepath: str) -> gpd.GeoDataFrame:\n",
    "        \"\"\"Load and clip vector data\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Warning: File not found: {filepath}\")\n",
    "            return gpd.GeoDataFrame(geometry=[], crs=self.crs)\n",
    "        \n",
    "        try:\n",
    "            gdf = gpd.read_file(filepath)\n",
    "            if gdf.crs != self.crs:\n",
    "                gdf = gdf.to_crs(self.crs)\n",
    "            return gdf[gdf.geometry.intersects(self.analysis_boundary)].copy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filepath}: {e}\")\n",
    "            return gpd.GeoDataFrame(geometry=[], crs=self.crs)\n",
    "    \n",
    "    def _load_and_align_raster(self, raster_path: str) -> np.ndarray:\n",
    "        \"\"\"Load and align raster to target grid\"\"\"\n",
    "        if not os.path.exists(raster_path):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with rasterio.open(raster_path) as src:\n",
    "                transform, width, height = calculate_default_transform(\n",
    "                    src.crs, self.crs, src.width, src.height,\n",
    "                    *src.bounds, resolution=self.target_resolution\n",
    "                )\n",
    "                \n",
    "                destination = np.zeros((height, width), dtype=src.dtypes[0])\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, 1),\n",
    "                    destination=destination,\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=self.crs,\n",
    "                    resampling=Resampling.bilinear\n",
    "                )\n",
    "                \n",
    "                # Clip to analysis bounds\n",
    "                minx, miny, maxx, maxy = self.bounds\n",
    "                row_start, col_start = rasterio.transform.rowcol(transform, minx, maxy)\n",
    "                row_end, col_end = rasterio.transform.rowcol(transform, maxx, miny)\n",
    "                \n",
    "                row_start, col_start = max(0, row_start), max(0, col_start)\n",
    "                row_end, col_end = min(height, row_end), min(width, col_end)\n",
    "                \n",
    "                clipped = destination[row_start:row_end, col_start:col_end]\n",
    "                \n",
    "                # Resize to target dimensions if needed\n",
    "                if clipped.shape != (self.height, self.width):\n",
    "                    from scipy.ndimage import zoom\n",
    "                    zoom_factors = (self.height/clipped.shape[0], \n",
    "                                  self.width/clipped.shape[1])\n",
    "                    return zoom(clipped, zoom_factors, order=0)\n",
    "                \n",
    "                return clipped\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading raster {raster_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compute_water_accessibility(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute water accessibility using pipe network, terrain, and population density.\n",
    "        Returns accessibility values for each OBD dataset.\n",
    "        \"\"\"\n",
    "        print(\"\\nComputing water accessibility...\")\n",
    "        \n",
    "        # Ensure all required data are available\n",
    "        if self.elevation is None or self.pipe_network is None or self.population is None:\n",
    "            print(\"Warning: Missing elevation, pipe network, or population data\")\n",
    "            # Return uniform accessibility as fallback\n",
    "            uniform_value = np.array([50] * 100)\n",
    "            return {\n",
    "                'Google': uniform_value.copy(),\n",
    "                'Microsoft': uniform_value.copy(),\n",
    "                'OSM': uniform_value.copy(),\n",
    "                'GBA': uniform_value.copy()\n",
    "            }\n",
    "        \n",
    "        height, width = self.height, self.width\n",
    "        from rasterio.features import geometry_mask\n",
    "        \n",
    "        # --- 1. Pipe proximity score (linear decay up to 2000 m) ---\n",
    "        # Create pipe source raster\n",
    "        pipe_raster = geometry_mask(\n",
    "            [geom for geom in self.pipe_network.geometry],\n",
    "            out_shape=(height, width),\n",
    "            transform=self.target_transform,\n",
    "            invert=True,\n",
    "            all_touched=True\n",
    "        ).astype(float)\n",
    "        \n",
    "        # Get coordinates of pipe cells\n",
    "        y_indices, x_indices = np.where(pipe_raster == 1)\n",
    "        if len(y_indices) == 0:\n",
    "            # Fallback: use center point\n",
    "            center_row, center_col = height // 2, width // 2\n",
    "            y_indices, x_indices = np.array([center_row]), np.array([center_col])\n",
    "        \n",
    "        # Create coordinate arrays for all cells\n",
    "        rows, cols = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
    "        xs, ys = rasterio.transform.xy(self.target_transform, rows.flatten(), cols.flatten())\n",
    "        coords = np.column_stack([np.array(xs), np.array(ys)])\n",
    "        \n",
    "        # Pipe coordinates\n",
    "        pipe_xs, pipe_ys = rasterio.transform.xy(self.target_transform, y_indices, x_indices)\n",
    "        pipe_coords = np.column_stack([np.array(pipe_xs), np.array(pipe_ys)])\n",
    "        \n",
    "        # Calculate Euclidean distances using KDTree\n",
    "        tree = cKDTree(pipe_coords)\n",
    "        distances = np.zeros(len(coords))\n",
    "        chunk_size = 10000\n",
    "        for i in range(0, len(coords), chunk_size):\n",
    "            chunk_end = min(i + chunk_size, len(coords))\n",
    "            chunk_dist, _ = tree.query(coords[i:chunk_end], k=1)\n",
    "            distances[i:chunk_end] = chunk_dist\n",
    "        distance_raster = distances.reshape((height, width))\n",
    "        \n",
    "        # Apply linear decay with fixed threshold d_max = 2000 m (from manuscript)\n",
    "        d_max = 2000.0\n",
    "        distance_score = np.maximum(0, 1 - distance_raster / d_max)\n",
    "        \n",
    "        # --- 2. Terrain slope score (exponential decay) ---\n",
    "        from scipy.ndimage import sobel\n",
    "        dz_dx = sobel(self.elevation, axis=1) / (2 * self.target_resolution)\n",
    "        dz_dy = sobel(self.elevation, axis=0) / (2 * self.target_resolution)\n",
    "        slope = np.degrees(np.arctan(np.sqrt(dz_dx**2 + dz_dy**2)))\n",
    "        \n",
    "        # Apply exponential decay f_s(s) = exp(-0.1 * s) (from manuscript)\n",
    "        slope_score = np.exp(-0.1 * slope)\n",
    "        \n",
    "        # --- 3. Population density score (min-max scaling) ---\n",
    "        pop = self.population.astype(float)\n",
    "        p_min = pop.min()\n",
    "        p_max = pop.max()\n",
    "        if p_max > p_min:\n",
    "            population_score = (pop - p_min) / (p_max - p_min)\n",
    "        else:\n",
    "            population_score = np.ones_like(pop)  # uniform if no variation\n",
    "        \n",
    "        # --- 4. Weighted combination (α=0.5, β=0.3, γ=0.2) ---\n",
    "        water_accessibility = (0.5 * distance_score +\n",
    "                            0.3 * slope_score +\n",
    "                            0.2 * population_score)\n",
    "        \n",
    "        # Scale to 0-100 and convert to uint8 for storage\n",
    "        water_accessibility = (water_accessibility * 100).astype(np.uint8)\n",
    "        \n",
    "        # --- 5. Extract values at building centroids for each dataset ---\n",
    "        results = {}\n",
    "        for name, buildings in [('Google', self.google_obd),\n",
    "                                ('Microsoft', self.microsoft_obd),\n",
    "                                ('OSM', self.osm_buildings),\n",
    "                                ('GBA', self.global_buildings)]:\n",
    "            if len(buildings) == 0:\n",
    "                results[name] = np.array([])\n",
    "                continue\n",
    "            \n",
    "            values = []\n",
    "            centroids = buildings.geometry.centroid\n",
    "            for centroid in centroids:\n",
    "                try:\n",
    "                    row, col = rasterio.transform.rowcol(self.target_transform,\n",
    "                                                        centroid.x, centroid.y)\n",
    "                    if 0 <= row < height and 0 <= col < width:\n",
    "                        values.append(water_accessibility[row, col])\n",
    "                    else:\n",
    "                        values.append(0)\n",
    "                except:\n",
    "                    values.append(0)\n",
    "            results[name] = np.array(values)\n",
    "        \n",
    "        # --- 6. Save outputs for mapping ---\n",
    "        # Create directory if not exists\n",
    "        os.makedirs(\"ranking_results\", exist_ok=True)\n",
    "        \n",
    "        # Save pipe network\n",
    "        pipe_path = \"ranking_results/pipe_network.geojson\"\n",
    "        self.pipe_network.to_file(pipe_path, driver='GeoJSON')\n",
    "        print(f\"Pipe network saved to {pipe_path}\")\n",
    "        \n",
    "        # Save water accessibility raster\n",
    "        output_raster_path = \"ranking_results/water_accessibility.tif\"\n",
    "        with rasterio.open(\n",
    "            output_raster_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=height,\n",
    "            width=width,\n",
    "            count=1,\n",
    "            dtype=water_accessibility.dtype,\n",
    "            crs=self.crs,                # corrected attribute name\n",
    "            transform=self.target_transform,   # corrected attribute name\n",
    "        ) as dst:\n",
    "            dst.write(water_accessibility, 1)\n",
    "        print(f\"Water accessibility raster saved to {output_raster_path}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compute_health_accessibility(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute health accessibility from pre-computed raster\n",
    "        Returns accessibility values for each OBD dataset\n",
    "        \"\"\"\n",
    "        print(\"\\nComputing health accessibility...\")\n",
    "        \n",
    "        if self.health_access is None:\n",
    "            print(\"Warning: Health accessibility raster not found\")\n",
    "            uniform_value = np.array([50] * 100)\n",
    "            return {\n",
    "                'Google': uniform_value.copy(),\n",
    "                'Microsoft': uniform_value.copy(),\n",
    "                'OSM': uniform_value.copy(),\n",
    "                'GBA': uniform_value.copy()  # ADDED\n",
    "            }\n",
    "        \n",
    "        # Scale health accessibility to 0-100\n",
    "        health_raster = self.health_access\n",
    "        if health_raster.max() > 0:\n",
    "            health_raster = (health_raster / health_raster.max() * 100).astype(np.uint8)\n",
    "        \n",
    "        height, width = health_raster.shape\n",
    "        results = {}\n",
    "        \n",
    "        # ADDED Global Building Atlas\n",
    "        for name, buildings in [('Google', self.google_obd),\n",
    "                               ('Microsoft', self.microsoft_obd),\n",
    "                               ('OSM', self.osm_buildings),\n",
    "                               ('GBA', self.global_buildings)]:\n",
    "            if len(buildings) == 0:\n",
    "                results[name] = np.array([])\n",
    "                continue\n",
    "            \n",
    "            values = []\n",
    "            centroids = buildings.geometry.centroid\n",
    "            \n",
    "            for centroid in centroids:\n",
    "                try:\n",
    "                    row, col = rasterio.transform.rowcol(self.target_transform, \n",
    "                                                        centroid.x, centroid.y)\n",
    "                    if 0 <= row < height and 0 <= col < width:\n",
    "                        values.append(health_raster[row, col])\n",
    "                    else:\n",
    "                        values.append(0)\n",
    "                except:\n",
    "                    values.append(0)\n",
    "            \n",
    "            results[name] = np.array(values)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_synthetic_reference(self, accessibility_values: np.ndarray, \n",
    "                                   n_samples: int = 1000) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate synthetic reference samples for TODQA\n",
    "        Based on the distribution of accessibility values\n",
    "        \"\"\"\n",
    "        if len(accessibility_values) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Filter out zero values\n",
    "        valid_values = accessibility_values[accessibility_values > 0]\n",
    "        \n",
    "        if len(valid_values) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        n_samples = min(n_samples, len(valid_values))\n",
    "        \n",
    "        # Use kernel density estimation or simple random sampling\n",
    "        if len(valid_values) >= n_samples:\n",
    "            indices = np.random.choice(len(valid_values), n_samples, replace=False)\n",
    "            synthetic = valid_values[indices]\n",
    "        else:\n",
    "            # If not enough samples, repeat with some noise\n",
    "            repeats = n_samples // len(valid_values) + 1\n",
    "            synthetic = np.tile(valid_values, repeats)[:n_samples]\n",
    "            # Add small noise\n",
    "            synthetic = synthetic + np.random.normal(0, 1, n_samples)\n",
    "            synthetic = np.clip(synthetic, 0, 100).astype(np.uint8)\n",
    "        \n",
    "        return synthetic\n",
    "\n",
    "\n",
    "class RankAggregation:\n",
    "    \"\"\"\n",
    "    2-Approximation Algorithm for Rank Aggregation\n",
    "    Based on Li et al. (2019) methodology\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rankings = {}\n",
    "        self.weights = {}\n",
    "        \n",
    "    def add_ranking(self, task_name: str, ranking: Dict[str, float], weight: float = 1.0):\n",
    "        \"\"\"\n",
    "        Add a ranking for a specific task\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        task_name : str, name of the task (e.g., 'health', 'water')\n",
    "        ranking : dict, mapping dataset names to scores (higher is better)\n",
    "        weight : float, weight for this task in aggregation\n",
    "        \"\"\"\n",
    "        self.rankings[task_name] = ranking\n",
    "        self.weights[task_name] = weight\n",
    "        \n",
    "    def compute_aggregated_ranking(self) -> Tuple[List[str], Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Compute aggregated ranking using weighted score summation\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        sorted_datasets : list, datasets sorted by aggregated score (best first)\n",
    "        aggregated_scores : dict, final aggregated scores for each dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.rankings:\n",
    "            raise ValueError(\"No rankings added\")\n",
    "        \n",
    "        # Get all unique datasets\n",
    "        all_datasets = set()\n",
    "        for ranking in self.rankings.values():\n",
    "            all_datasets.update(ranking.keys())\n",
    "        all_datasets = list(all_datasets)\n",
    "        \n",
    "        # Line 1: Initialize score vector r[dataset] ← 0 for each dataset\n",
    "        r = {dataset: 0.0 for dataset in all_datasets}\n",
    "        \n",
    "        # Lines 2-6: For each dataset, for each task, add weighted score\n",
    "        for dataset in all_datasets:  # Line 2\n",
    "            for task_name, ranking in self.rankings.items():  # Line 3\n",
    "                weight = self.weights.get(task_name, 1.0)\n",
    "                # τ_d(t) is the score of dataset d for task t\n",
    "                # Use 0 if dataset not in this task's ranking\n",
    "                task_score = ranking.get(dataset, 0.0)\n",
    "                # Line 4: r[d] ← r[d] + μ_t × τ_d(t)\n",
    "                r[dataset] += weight * task_score  # Line 4\n",
    "            # Line 5-6: End for loops\n",
    "        \n",
    "        # Line 7: Sort datasets by descending values of r[d]\n",
    "        # Create list of (dataset, score) tuples\n",
    "        dataset_scores = list(r.items())\n",
    "        # Sort by score in descending order (higher score = better)\n",
    "        sorted_dataset_scores = sorted(dataset_scores, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Extract sorted dataset names\n",
    "        sorted_datasets = [dataset for dataset, _ in sorted_dataset_scores]\n",
    "        \n",
    "        # Return aggregated scores dictionary\n",
    "        aggregated_scores = {dataset: score for dataset, score in sorted_dataset_scores}\n",
    "        \n",
    "        # Line 8: Return σ (represented as sorted_datasets)\n",
    "        return sorted_datasets, aggregated_scores\n",
    "    \n",
    "    def compute_task_relevancy_scores(self, assessor,\n",
    "                                     D_features: Dict[str, np.ndarray],\n",
    "                                     S_features: Dict[str, np.ndarray],\n",
    "                                     task_type: str = 'accessibility') -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute task relevancy scores for all datasets using TODQA\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        assessor : TODQAAssessor instance\n",
    "        D_features : dict mapping dataset names to feature arrays\n",
    "        S_features : dict mapping task names to synthetic reference feature arrays\n",
    "        task_type : type of similarity function to use\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        scores : dict mapping dataset names to task relevancy scores\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for dataset_name, features in D_features.items():\n",
    "            if len(features) == 0:\n",
    "                scores[dataset_name] = 0.0\n",
    "                continue\n",
    "            \n",
    "            # Average score across all task references\n",
    "            task_scores = []\n",
    "            for task_name, s_features in S_features.items():\n",
    "                if len(s_features) > 0:\n",
    "                    q_score = assessor.compute_relevancy(\n",
    "                        features.reshape(-1, 1),  # Accessibility is 1D feature\n",
    "                        s_features.reshape(-1, 1),\n",
    "                        task_type=task_type\n",
    "                    )\n",
    "                    task_scores.append(q_score)\n",
    "            \n",
    "            scores[dataset_name] = np.mean(task_scores) if task_scores else 0.0\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d70738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "OBD DATASET RANKING PIPELINE\n",
      "Datasets: Google, Microsoft, OSM, GlobalBuildingMap\n",
      "Tasks: 1. Health Facility Accessibility, 2. Clean Water Accessibility\n",
      "======================================================================\n",
      "\n",
      "Step 1: Loading data...\n",
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1450097/640231440.py:195: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  centroid = self.boundary.unary_union.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: Google=91331, Microsoft=44695, OSM=31404, GBA=68090\n",
      "Boundary saved to ranking_results/ashaiman_boundary.geojson\n",
      "\n",
      "Step 2: Computing accessibility metrics...\n",
      "\n",
      "Computing health accessibility...\n",
      "  Health accessibility computed:\n",
      "    Google: 91331 buildings, mean=43.5\n",
      "    Microsoft: 44695 buildings, mean=42.1\n",
      "    OSM: 31404 buildings, mean=50.1\n",
      "    GBA: 68090 buildings, mean=44.8\n",
      "\n",
      "Computing water accessibility...\n",
      "Pipe network saved to ranking_results/pipe_network.geojson\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AccessibilityPipeline' object has no attribute 'target_crs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 220\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# Run main pipeline\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: No buildings found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Water accessibility\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m water_results \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_water_accessibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Water accessibility computed:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset, values \u001b[38;5;129;01min\u001b[39;00m water_results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[4], line 441\u001b[0m, in \u001b[0;36mAccessibilityPipeline.compute_water_accessibility\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    430\u001b[0m output_raster_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mranking_results/water_accessibility.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mranking_results\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m rasterio\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m    434\u001b[0m     output_raster_path,\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    436\u001b[0m     driver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGTiff\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    437\u001b[0m     height\u001b[38;5;241m=\u001b[39mheight,\n\u001b[1;32m    438\u001b[0m     width\u001b[38;5;241m=\u001b[39mwidth,\n\u001b[1;32m    439\u001b[0m     count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    440\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mwater_accessibility\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m--> 441\u001b[0m     crs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_crs\u001b[49m,\n\u001b[1;32m    442\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtarget_transform,\n\u001b[1;32m    443\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m dst:\n\u001b[1;32m    444\u001b[0m     dst\u001b[38;5;241m.\u001b[39mwrite(water_accessibility, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWater accessibility raster saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_raster_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)            \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AccessibilityPipeline' object has no attribute 'target_crs'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution pipeline:\n",
    "    1. Compute health and water accessibility for four OBD datasets\n",
    "    2. Generate synthetic references\n",
    "    3. Compute task relevancy scores using TODQA\n",
    "    4. Aggregate rankings using 2-Approximation Algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"OBD DATASET RANKING PIPELINE\")\n",
    "    print(\"Datasets: Google, Microsoft, OSM, GlobalBuildingMap\")\n",
    "    print(\"Tasks: 1. Health Facility Accessibility, 2. Clean Water Accessibility\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = AccessibilityPipeline(city=\"accra\", target_resolution=30)\n",
    "    \n",
    "    # Step 1: Load data (now includes Global Building Atlas)\n",
    "    print(\"\\nStep 1: Loading data...\")\n",
    "    pipeline.load_data()\n",
    "    \n",
    "    # Step 2: Compute accessibility metrics for all four datasets\n",
    "    print(\"\\nStep 2: Computing accessibility metrics...\")\n",
    "    \n",
    "    # Health accessibility\n",
    "    health_results = pipeline.compute_health_accessibility()\n",
    "    print(f\"  Health accessibility computed:\")\n",
    "    for dataset, values in health_results.items():\n",
    "        if len(values) > 0:\n",
    "            print(f\"    {dataset}: {len(values)} buildings, mean={values.mean():.1f}\")\n",
    "        else:\n",
    "            print(f\"    {dataset}: No buildings found\")\n",
    "    \n",
    "    # Water accessibility\n",
    "    water_results = pipeline.compute_water_accessibility()\n",
    "    print(f\"  Water accessibility computed:\")\n",
    "    for dataset, values in water_results.items():\n",
    "        if len(values) > 0:\n",
    "            print(f\"    {dataset}: {len(values)} buildings, mean={values.mean():.1f}\")\n",
    "        else:\n",
    "            print(f\"    {dataset}: No buildings found\")\n",
    "    \n",
    "    # Step 3: Generate synthetic references\n",
    "    print(\"\\nStep 3: Generating synthetic references...\")\n",
    "    \n",
    "    # Combine all accessibility values to create reference distribution\n",
    "    all_health_values = np.concatenate([v for v in health_results.values() if len(v) > 0])\n",
    "    all_water_values = np.concatenate([v for v in water_results.values() if len(v) > 0])\n",
    "    \n",
    "    health_reference = pipeline.generate_synthetic_reference(all_health_values, 1000)\n",
    "    water_reference = pipeline.generate_synthetic_reference(all_water_values, 1000)\n",
    "    \n",
    "    print(f\"  Synthetic references: Health={len(health_reference)}, Water={len(water_reference)}\")\n",
    "    \n",
    "    # Step 4: Compute task relevancy scores using TODQA\n",
    "    print(\"\\nStep 4: Computing task relevancy scores (TODQA)...\")\n",
    "    \n",
    "    # Prepare feature dictionaries for all four datasets\n",
    "    D_features_health = {\n",
    "        'Google': health_results.get('Google', np.array([])),\n",
    "        'Microsoft': health_results.get('Microsoft', np.array([])),\n",
    "        'OSM': health_results.get('OSM', np.array([])),\n",
    "        'GBA': health_results.get('GBA', np.array([]))  # ADDED\n",
    "    }\n",
    "    \n",
    "    D_features_water = {\n",
    "        'Google': water_results.get('Google', np.array([])),\n",
    "        'Microsoft': water_results.get('Microsoft', np.array([])),\n",
    "        'OSM': water_results.get('OSM', np.array([])),\n",
    "        'GBA': water_results.get('GBA', np.array([]))  # ADDED\n",
    "    }\n",
    "    \n",
    "    S_features = {\n",
    "        'health': health_reference,\n",
    "        'water': water_reference\n",
    "    }\n",
    "    \n",
    "    # Initialize TODQA assessor\n",
    "    todqa = TODQAAssessor(k=16)\n",
    "    \n",
    "    # Compute health accessibility scores for all four datasets\n",
    "    health_scores = {}\n",
    "    for dataset, features in D_features_health.items():\n",
    "        if len(features) > 0 and len(health_reference) > 0:\n",
    "            q_score = todqa.compute_relevancy(\n",
    "                features.reshape(-1, 1),\n",
    "                health_reference.reshape(-1, 1),\n",
    "                task_type='accessibility'\n",
    "            )\n",
    "            health_scores[dataset] = q_score\n",
    "        else:\n",
    "            health_scores[dataset] = 0.0\n",
    "    \n",
    "    print(\"  Health accessibility relevancy scores:\")\n",
    "    for dataset, score in health_scores.items():\n",
    "        print(f\"    {dataset}: {score:.4f}\")\n",
    "    \n",
    "    # Compute water accessibility scores for all four datasets\n",
    "    water_scores = {}\n",
    "    for dataset, features in D_features_water.items():\n",
    "        if len(features) > 0 and len(water_reference) > 0:\n",
    "            q_score = todqa.compute_relevancy(\n",
    "                features.reshape(-1, 1),\n",
    "                water_reference.reshape(-1, 1),\n",
    "                task_type='accessibility'\n",
    "            )\n",
    "            water_scores[dataset] = q_score\n",
    "        else:\n",
    "            water_scores[dataset] = 0.0\n",
    "    \n",
    "    print(\"  Water accessibility relevancy scores:\")\n",
    "    for dataset, score in water_scores.items():\n",
    "        print(f\"    {dataset}: {score:.4f}\")\n",
    "    \n",
    "    # Step 5: Aggregate rankings using 2-Approximation Algorithm\n",
    "    print(\"\\nStep 5: Aggregating rankings (2-Approximation Algorithm)...\")\n",
    "    \n",
    "    rank_aggregator = RankAggregation()\n",
    "    \n",
    "    # Add rankings with weights\n",
    "    health_weight = 0.5\n",
    "    water_weight = 0.5\n",
    "    \n",
    "    rank_aggregator.add_ranking('health_accessibility', health_scores, health_weight)\n",
    "    rank_aggregator.add_ranking('water_accessibility', water_scores, water_weight)\n",
    "    \n",
    "    # Compute aggregated ranking for all four datasets\n",
    "    sorted_datasets, aggregated_scores = rank_aggregator.compute_aggregated_ranking()\n",
    "    \n",
    "    # Step 6: Display results\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL RANKING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nIndividual Task Rankings:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\n1. Health Facility Accessibility:\")\n",
    "    health_ranked = sorted(health_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for rank, (dataset, score) in enumerate(health_ranked, 1):\n",
    "        print(f\"   {rank}. {dataset}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\n2. Clean Water Accessibility:\")\n",
    "    water_ranked = sorted(water_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for rank, (dataset, score) in enumerate(water_ranked, 1):\n",
    "        print(f\"   {rank}. {dataset}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Aggregated Ranking (2-Approximation Algorithm):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for rank, dataset in enumerate(sorted_datasets, 1):\n",
    "        score = aggregated_scores[dataset]\n",
    "        print(f\"   {rank}. {dataset}: aggregated score = {score:.4f}\")\n",
    "    \n",
    "    # Calculate and display final normalized scores\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"Final Normalized Scores (0-1):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Normalize aggregated scores to 0-1 range\n",
    "    min_score = min(aggregated_scores.values())\n",
    "    max_score = max(aggregated_scores.values())\n",
    "    \n",
    "    if max_score != min_score:\n",
    "        normalized_scores = {\n",
    "            dataset: (score - min_score) / (max_score - min_score)\n",
    "            for dataset, score in aggregated_scores.items()\n",
    "        }\n",
    "    else:\n",
    "        normalized_scores = {dataset: 1.0 for dataset in aggregated_scores.keys()}\n",
    "    \n",
    "    for dataset, score in sorted(normalized_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   {dataset}: {score:.4f}\")\n",
    "    \n",
    "    # Step 7: Save results\n",
    "    print(\"\\nSaving results...\")\n",
    "    results = {\n",
    "        'task_scores': {\n",
    "            'health_accessibility': health_scores,\n",
    "            'water_accessibility': water_scores\n",
    "        },\n",
    "        'weights': {\n",
    "            'health_accessibility': health_weight,\n",
    "            'water_accessibility': water_weight\n",
    "        },\n",
    "        'aggregated_ranking': sorted_datasets,\n",
    "        'aggregated_scores': aggregated_scores,\n",
    "        'normalized_scores': normalized_scores\n",
    "    }\n",
    "    \n",
    "    os.makedirs(\"ranking_results\", exist_ok=True)\n",
    "    with open(\"ranking_results/obd_ranking_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Save detailed accessibility values\n",
    "    accessibility_data = {\n",
    "        'health_values': {k: v.tolist() for k, v in health_results.items() if len(v) > 0},\n",
    "        'water_values': {k: v.tolist() for k, v in water_results.items() if len(v) > 0},\n",
    "        'synthetic_references': {\n",
    "            'health': health_reference.tolist() if len(health_reference) > 0 else [],\n",
    "            'water': water_reference.tolist() if len(water_reference) > 0 else []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(\"ranking_results/accessibility_values.json\", \"w\") as f:\n",
    "        json.dump(accessibility_data, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"Results saved to 'ranking_results' directory\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run main pipeline\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d83d4d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_scores': {'health_accessibility': {'Google': np.float64(0.9726593385746379),\n",
       "   'Microsoft': np.float64(0.9204352781701027),\n",
       "   'OSM': np.float64(0.7268322122880696),\n",
       "   'Global': np.float64(0.8322988661317204)},\n",
       "  'water_accessibility': {'Google': np.float64(0.9753663515705194),\n",
       "   'Microsoft': np.float64(0.9428226666663616),\n",
       "   'OSM': np.float64(0.8947612805457444),\n",
       "   'Global': np.float64(0.8696571161434651)}},\n",
       " 'weights': {'health_accessibility': 0.5, 'water_accessibility': 0.5},\n",
       " 'aggregated_ranking': ['Google', 'Microsoft', 'Global', 'OSM'],\n",
       " 'aggregated_scores': {'Google': np.float64(0.9740128450725787),\n",
       "  'Microsoft': np.float64(0.9316289724182322),\n",
       "  'Global': np.float64(0.8509779911375928),\n",
       "  'OSM': np.float64(0.810796746416907)},\n",
       " 'normalized_scores': {'Google': np.float64(1.0),\n",
       "  'Microsoft': np.float64(0.7403205137027474),\n",
       "  'Global': np.float64(0.24618432281887817),\n",
       "  'OSM': np.float64(0.0)}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
